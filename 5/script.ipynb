{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#import timeit\n",
    "import json\n",
    "import os\n",
    "import numpy\n",
    "\n",
    "from my_callback import MyCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.spaces\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "LAG = 3\n",
    "\n",
    "def calc_profit(action, df, index):\n",
    "    if action == 0: # long\n",
    "        p = 1\n",
    "    elif action == 1: # short\n",
    "        p = -1\n",
    "    else: # stay\n",
    "        p = 0\n",
    "    return  p * df[\"c\"][index]\n",
    "\n",
    "def calc_observation(df, index, columns):\n",
    "    return numpy.array(\n",
    "        [\n",
    "            [df[col][index-t] for col in columns] for t in range(LAG)\n",
    "        ] \n",
    "    )\n",
    "\n",
    "class Game(gym.core.Env):\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "    def __init__(self, df, columns):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.columns = columns\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.observation_space = gym.spaces.Box(0, 999, shape=(LAG, len(columns)))\n",
    "        self.time = LAG\n",
    "        self.profit = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = calc_profit(action, self.df, self.time)\n",
    "        self.time += 1\n",
    "        self.profit += reward       \n",
    "        done = self.time == (len(self.df) - 1)\n",
    "        if done:\n",
    "            print(\"profit___{}\".format(self.profit))\n",
    "        info = {}\n",
    "        observation = calc_observation(self.df, self.time, self.columns)\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.time = LAG\n",
    "        self.profit = 0\n",
    "        return calc_observation(self.df, self.time, self.columns)\n",
    "    \n",
    "    def render(self, mode):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def seed(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Activation,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    concatenate,\n",
    "    Dropout,\n",
    "    LSTM,\n",
    "    Reshape\n",
    ")\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def sample_model(self, observation_space):\n",
    "        model = Sequential()\n",
    "        model.add(Reshape(observation_space.shape,\n",
    "                      input_shape=(1,)+observation_space.shape))\n",
    "        model.add(LSTM(LAG))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "        #model.add(Dropout(0.6))\n",
    "        model.add(Dense(16))\n",
    "        model.add(Activation('relu'))\n",
    "        #model.add(Dropout(0.6))\n",
    "        model.add(Dense(n_action))\n",
    "        model.add(Activation('linear'))\n",
    "        self.model = model\n",
    "        return model        \n",
    "    \n",
    "    def from_json(self, file_path):\n",
    "        pass\n",
    "    \n",
    "    def to_json(self, output_path):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def agent(model, n_action):\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    policy = EpsGreedyQPolicy(eps=0.1)\n",
    "    dqn_agent = DQNAgent(model=model, nb_actions=n_action,\n",
    "                         memory=memory, nb_steps_warmup=100,\n",
    "                         target_model_update=1e-2, policy=policy)\n",
    "    dqn_agent.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "    return dqn_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "df = pandas.DataFrame({\"a\": numpy.random.rand(1000), \"b\": numpy.random.rand(1000)})\n",
    "df[\"c\"]= ((df[\"a\"] - df[\"b\"]).shift(2)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064463</td>\n",
       "      <td>0.441743</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649705</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.372289</td>\n",
       "      <td>0.749011</td>\n",
       "      <td>-0.377280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.190842</td>\n",
       "      <td>0.181702</td>\n",
       "      <td>0.630703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.510036</td>\n",
       "      <td>0.970349</td>\n",
       "      <td>-0.376723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.223740</td>\n",
       "      <td>0.027363</td>\n",
       "      <td>0.009140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.532533</td>\n",
       "      <td>0.801741</td>\n",
       "      <td>-0.460313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.635256</td>\n",
       "      <td>0.056010</td>\n",
       "      <td>0.196377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.008362</td>\n",
       "      <td>0.570236</td>\n",
       "      <td>-0.269208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.240967</td>\n",
       "      <td>0.896362</td>\n",
       "      <td>0.579246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c\n",
       "0  0.064463  0.441743  0.000000\n",
       "1  0.649705  0.019002  0.000000\n",
       "2  0.372289  0.749011 -0.377280\n",
       "3  0.190842  0.181702  0.630703\n",
       "4  0.510036  0.970349 -0.376723\n",
       "5  0.223740  0.027363  0.009140\n",
       "6  0.532533  0.801741 -0.460313\n",
       "7  0.635256  0.056010  0.196377\n",
       "8  0.008362  0.570236 -0.269208\n",
       "9  0.240967  0.896362  0.579246"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "columns = [\"a\", \"b\"]\n",
    "env = Game(df, columns)\n",
    "n_action = 3\n",
    "network = Network()\n",
    "model = network.sample_model(env.observation_space)\n",
    "agent_v6 = agent(model, n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Training for 100000 steps ...\n",
      "profit___262.2135627716549\n",
      "   996/100000: episode: 1, duration: 4.679s, episode steps: 996, steps per second: 213, episode reward: 262.214, mean reward: 0.263 [-0.923, 0.978], mean action: 0.583 [0.000, 2.000], loss: 0.028139, mean_absolute_error: 0.606626, mean_q: 1.204131\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      "   996/100000: episode: 1, duration: 4.726s, episode steps: 996, steps per second: 211, episode reward: 262.214, mean reward: 0.263 [-0.923, 0.978], mean action: 0.583 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.028139, mean_absolute_error: 0.606626, mean_q: 1.204131\n",
      "profit___299.5220836642925\n",
      "  1992/100000: episode: 2, duration: 3.673s, episode steps: 996, steps per second: 271, episode reward: 299.522, mean reward: 0.301 [-0.769, 0.971], mean action: 0.548 [0.000, 2.000], loss: 0.031391, mean_absolute_error: 2.282548, mean_q: 3.799711\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      "  1992/100000: episode: 2, duration: 3.683s, episode steps: 996, steps per second: 270, episode reward: 299.522, mean reward: 0.301 [-0.769, 0.971], mean action: 0.548 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.031391, mean_absolute_error: 2.282548, mean_q: 3.799711\n",
      "profit___288.28743133368795\n",
      "  2988/100000: episode: 3, duration: 3.705s, episode steps: 996, steps per second: 269, episode reward: 288.287, mean reward: 0.289 [-0.933, 0.978], mean action: 0.564 [0.000, 2.000], loss: 0.037519, mean_absolute_error: 4.070291, mean_q: 6.452659\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  2988/100000: episode: 3, duration: 3.706s, episode steps: 996, steps per second: 269, episode reward: 288.287, mean reward: 0.289 [-0.933, 0.978], mean action: 0.564 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.037519, mean_absolute_error: 4.070291, mean_q: 6.452659\n",
      "profit___298.0423948955588\n",
      "  3984/100000: episode: 4, duration: 3.638s, episode steps: 996, steps per second: 274, episode reward: 298.042, mean reward: 0.299 [-0.933, 0.978], mean action: 0.556 [0.000, 2.000], loss: 0.042456, mean_absolute_error: 5.669501, mean_q: 8.829865\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  3984/100000: episode: 4, duration: 3.639s, episode steps: 996, steps per second: 274, episode reward: 298.042, mean reward: 0.299 [-0.933, 0.978], mean action: 0.556 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.042456, mean_absolute_error: 5.669501, mean_q: 8.829865\n",
      "profit___290.67058822947274\n",
      "  4980/100000: episode: 5, duration: 3.637s, episode steps: 996, steps per second: 274, episode reward: 290.671, mean reward: 0.292 [-0.859, 0.978], mean action: 0.589 [0.000, 2.000], loss: 0.070333, mean_absolute_error: 7.128718, mean_q: 10.993342\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  4980/100000: episode: 5, duration: 3.638s, episode steps: 996, steps per second: 274, episode reward: 290.671, mean reward: 0.292 [-0.859, 0.978], mean action: 0.589 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.070333, mean_absolute_error: 7.128718, mean_q: 10.993342\n",
      "profit___292.08170200889344\n",
      "  5976/100000: episode: 6, duration: 3.575s, episode steps: 996, steps per second: 279, episode reward: 292.082, mean reward: 0.293 [-0.941, 0.978], mean action: 0.603 [0.000, 2.000], loss: 0.092655, mean_absolute_error: 8.381811, mean_q: 12.843488\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  5976/100000: episode: 6, duration: 3.577s, episode steps: 996, steps per second: 278, episode reward: 292.082, mean reward: 0.293 [-0.941, 0.978], mean action: 0.603 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.092655, mean_absolute_error: 8.381811, mean_q: 12.843488\n",
      "profit___296.8104971980688\n",
      "  6972/100000: episode: 7, duration: 3.598s, episode steps: 996, steps per second: 277, episode reward: 296.810, mean reward: 0.298 [-0.978, 0.971], mean action: 0.612 [0.000, 2.000], loss: 0.134410, mean_absolute_error: 9.468952, mean_q: 14.446517\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  6972/100000: episode: 7, duration: 3.599s, episode steps: 996, steps per second: 277, episode reward: 296.810, mean reward: 0.298 [-0.978, 0.971], mean action: 0.612 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.134410, mean_absolute_error: 9.468952, mean_q: 14.446517\n",
      "profit___285.3235846855055\n",
      "  7968/100000: episode: 8, duration: 3.577s, episode steps: 996, steps per second: 278, episode reward: 285.324, mean reward: 0.286 [-0.933, 0.978], mean action: 0.580 [0.000, 2.000], loss: 0.134450, mean_absolute_error: 10.448216, mean_q: 15.913883\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  7968/100000: episode: 8, duration: 3.578s, episode steps: 996, steps per second: 278, episode reward: 285.324, mean reward: 0.286 [-0.933, 0.978], mean action: 0.580 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.134450, mean_absolute_error: 10.448216, mean_q: 15.913883\n",
      "profit___282.6831868509054\n",
      "  8964/100000: episode: 9, duration: 3.676s, episode steps: 996, steps per second: 271, episode reward: 282.683, mean reward: 0.284 [-0.784, 0.978], mean action: 0.610 [0.000, 2.000], loss: 0.200009, mean_absolute_error: 11.336656, mean_q: 17.235584\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  8964/100000: episode: 9, duration: 3.677s, episode steps: 996, steps per second: 271, episode reward: 282.683, mean reward: 0.284 [-0.784, 0.978], mean action: 0.610 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.200009, mean_absolute_error: 11.336656, mean_q: 17.235584\n",
      "profit___285.1444316734603\n",
      "  9960/100000: episode: 10, duration: 3.591s, episode steps: 996, steps per second: 277, episode reward: 285.144, mean reward: 0.286 [-0.945, 0.978], mean action: 0.591 [0.000, 2.000], loss: 0.114885, mean_absolute_error: 12.139759, mean_q: 18.448664\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  9960/100000: episode: 10, duration: 3.592s, episode steps: 996, steps per second: 277, episode reward: 285.144, mean reward: 0.286 [-0.945, 0.978], mean action: 0.591 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.114885, mean_absolute_error: 12.139759, mean_q: 18.448664\n",
      "profit___298.2409865997381\n",
      " 10956/100000: episode: 11, duration: 3.564s, episode steps: 996, steps per second: 279, episode reward: 298.241, mean reward: 0.299 [-0.763, 0.978], mean action: 0.600 [0.000, 2.000], loss: 0.193821, mean_absolute_error: 12.855892, mean_q: 19.510174\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 10956/100000: episode: 11, duration: 3.565s, episode steps: 996, steps per second: 279, episode reward: 298.241, mean reward: 0.299 [-0.763, 0.978], mean action: 0.600 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.193821, mean_absolute_error: 12.855892, mean_q: 19.510174\n",
      "profit___287.44693527312677\n",
      " 11952/100000: episode: 12, duration: 3.672s, episode steps: 996, steps per second: 271, episode reward: 287.447, mean reward: 0.289 [-0.646, 0.978], mean action: 0.594 [0.000, 2.000], loss: 0.263805, mean_absolute_error: 13.473832, mean_q: 20.420355\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 11952/100000: episode: 12, duration: 3.673s, episode steps: 996, steps per second: 271, episode reward: 287.447, mean reward: 0.289 [-0.646, 0.978], mean action: 0.594 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.263805, mean_absolute_error: 13.473832, mean_q: 20.420355\n",
      "profit___279.0419892477198\n",
      " 12948/100000: episode: 13, duration: 3.669s, episode steps: 996, steps per second: 271, episode reward: 279.042, mean reward: 0.280 [-0.756, 0.978], mean action: 0.620 [0.000, 2.000], loss: 0.276630, mean_absolute_error: 13.988618, mean_q: 21.195055\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 12948/100000: episode: 13, duration: 3.670s, episode steps: 996, steps per second: 271, episode reward: 279.042, mean reward: 0.280 [-0.756, 0.978], mean action: 0.620 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.276630, mean_absolute_error: 13.988618, mean_q: 21.195055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit___279.8973996136567\n",
      " 13944/100000: episode: 14, duration: 3.727s, episode steps: 996, steps per second: 267, episode reward: 279.897, mean reward: 0.281 [-0.941, 0.971], mean action: 0.602 [0.000, 2.000], loss: 0.230040, mean_absolute_error: 14.489065, mean_q: 21.947218\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 13944/100000: episode: 14, duration: 3.728s, episode steps: 996, steps per second: 267, episode reward: 279.897, mean reward: 0.281 [-0.941, 0.971], mean action: 0.602 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.230040, mean_absolute_error: 14.489065, mean_q: 21.947218\n",
      "profit___276.5946113535709\n",
      " 14940/100000: episode: 15, duration: 3.700s, episode steps: 996, steps per second: 269, episode reward: 276.595, mean reward: 0.278 [-0.719, 0.978], mean action: 0.640 [0.000, 2.000], loss: 0.352726, mean_absolute_error: 14.894975, mean_q: 22.547369\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 14940/100000: episode: 15, duration: 3.701s, episode steps: 996, steps per second: 269, episode reward: 276.595, mean reward: 0.278 [-0.719, 0.978], mean action: 0.640 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.352726, mean_absolute_error: 14.894975, mean_q: 22.547369\n",
      "profit___277.72801874815076\n",
      " 15936/100000: episode: 16, duration: 3.749s, episode steps: 996, steps per second: 266, episode reward: 277.728, mean reward: 0.279 [-0.932, 0.978], mean action: 0.624 [0.000, 2.000], loss: 0.347716, mean_absolute_error: 15.255772, mean_q: 23.089487\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 15936/100000: episode: 16, duration: 3.750s, episode steps: 996, steps per second: 266, episode reward: 277.728, mean reward: 0.279 [-0.932, 0.978], mean action: 0.624 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.347716, mean_absolute_error: 15.255772, mean_q: 23.089487\n",
      "profit___287.35879800757317\n",
      " 16932/100000: episode: 17, duration: 3.724s, episode steps: 996, steps per second: 267, episode reward: 287.359, mean reward: 0.289 [-0.978, 0.971], mean action: 0.639 [0.000, 2.000], loss: 0.347236, mean_absolute_error: 15.585672, mean_q: 23.591404\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 16932/100000: episode: 17, duration: 3.725s, episode steps: 996, steps per second: 267, episode reward: 287.359, mean reward: 0.289 [-0.978, 0.971], mean action: 0.639 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.347236, mean_absolute_error: 15.585672, mean_q: 23.591404\n",
      "profit___282.2920490235132\n",
      " 17928/100000: episode: 18, duration: 3.691s, episode steps: 996, steps per second: 270, episode reward: 282.292, mean reward: 0.283 [-0.839, 0.978], mean action: 0.632 [0.000, 2.000], loss: 0.321370, mean_absolute_error: 15.895468, mean_q: 24.058617\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 17928/100000: episode: 18, duration: 3.692s, episode steps: 996, steps per second: 270, episode reward: 282.292, mean reward: 0.283 [-0.839, 0.978], mean action: 0.632 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.321370, mean_absolute_error: 15.895468, mean_q: 24.058617\n",
      "profit___287.1202290131209\n",
      " 18924/100000: episode: 19, duration: 3.673s, episode steps: 996, steps per second: 271, episode reward: 287.120, mean reward: 0.288 [-0.691, 0.978], mean action: 0.626 [0.000, 2.000], loss: 0.289491, mean_absolute_error: 16.191473, mean_q: 24.518229\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 18924/100000: episode: 19, duration: 3.674s, episode steps: 996, steps per second: 271, episode reward: 287.120, mean reward: 0.288 [-0.691, 0.978], mean action: 0.626 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.289491, mean_absolute_error: 16.191473, mean_q: 24.518229\n",
      "profit___281.24965934391975\n",
      " 19920/100000: episode: 20, duration: 3.772s, episode steps: 996, steps per second: 264, episode reward: 281.250, mean reward: 0.282 [-0.889, 0.978], mean action: 0.612 [0.000, 2.000], loss: 0.324871, mean_absolute_error: 16.460922, mean_q: 24.927410\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 19920/100000: episode: 20, duration: 3.774s, episode steps: 996, steps per second: 264, episode reward: 281.250, mean reward: 0.282 [-0.889, 0.978], mean action: 0.612 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.324871, mean_absolute_error: 16.460922, mean_q: 24.927410\n",
      "profit___293.7067518306083\n",
      " 20916/100000: episode: 21, duration: 4.221s, episode steps: 996, steps per second: 236, episode reward: 293.707, mean reward: 0.295 [-0.857, 0.971], mean action: 0.600 [0.000, 2.000], loss: 0.287549, mean_absolute_error: 16.704039, mean_q: 25.308458\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 20916/100000: episode: 21, duration: 4.222s, episode steps: 996, steps per second: 236, episode reward: 293.707, mean reward: 0.295 [-0.857, 0.971], mean action: 0.600 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.287549, mean_absolute_error: 16.704039, mean_q: 25.308458\n",
      "profit___288.40774047072637\n",
      " 21912/100000: episode: 22, duration: 3.542s, episode steps: 996, steps per second: 281, episode reward: 288.408, mean reward: 0.290 [-0.613, 0.978], mean action: 0.646 [0.000, 2.000], loss: 0.385801, mean_absolute_error: 16.925085, mean_q: 25.631098\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 21912/100000: episode: 22, duration: 3.543s, episode steps: 996, steps per second: 281, episode reward: 288.408, mean reward: 0.290 [-0.613, 0.978], mean action: 0.646 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.385801, mean_absolute_error: 16.925085, mean_q: 25.631098\n",
      "profit___270.7297298768718\n",
      " 22908/100000: episode: 23, duration: 3.473s, episode steps: 996, steps per second: 287, episode reward: 270.730, mean reward: 0.272 [-0.978, 0.971], mean action: 0.669 [0.000, 2.000], loss: 0.313256, mean_absolute_error: 17.109955, mean_q: 25.928322\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 22908/100000: episode: 23, duration: 3.474s, episode steps: 996, steps per second: 287, episode reward: 270.730, mean reward: 0.272 [-0.978, 0.971], mean action: 0.669 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.313256, mean_absolute_error: 17.109955, mean_q: 25.928322\n",
      "profit___286.4484383984453\n",
      " 23904/100000: episode: 24, duration: 3.386s, episode steps: 996, steps per second: 294, episode reward: 286.448, mean reward: 0.288 [-0.759, 0.978], mean action: 0.646 [0.000, 2.000], loss: 0.306865, mean_absolute_error: 17.327538, mean_q: 26.258945\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 23904/100000: episode: 24, duration: 3.387s, episode steps: 996, steps per second: 294, episode reward: 286.448, mean reward: 0.288 [-0.759, 0.978], mean action: 0.646 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.306865, mean_absolute_error: 17.327538, mean_q: 26.258945\n",
      "profit___275.6666257143674\n",
      " 24900/100000: episode: 25, duration: 3.382s, episode steps: 996, steps per second: 295, episode reward: 275.667, mean reward: 0.277 [-0.978, 0.971], mean action: 0.650 [0.000, 2.000], loss: 0.285226, mean_absolute_error: 17.527958, mean_q: 26.554350\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 24900/100000: episode: 25, duration: 3.383s, episode steps: 996, steps per second: 294, episode reward: 275.667, mean reward: 0.277 [-0.978, 0.971], mean action: 0.650 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.285226, mean_absolute_error: 17.527958, mean_q: 26.554350\n",
      "profit___282.43409707380306\n",
      " 25896/100000: episode: 26, duration: 3.391s, episode steps: 996, steps per second: 294, episode reward: 282.434, mean reward: 0.284 [-0.581, 0.978], mean action: 0.695 [0.000, 2.000], loss: 0.389342, mean_absolute_error: 17.677013, mean_q: 26.764021\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 25896/100000: episode: 26, duration: 3.392s, episode steps: 996, steps per second: 294, episode reward: 282.434, mean reward: 0.284 [-0.581, 0.978], mean action: 0.695 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.389342, mean_absolute_error: 17.677013, mean_q: 26.764021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit___284.21922660914703\n",
      " 26892/100000: episode: 27, duration: 3.351s, episode steps: 996, steps per second: 297, episode reward: 284.219, mean reward: 0.285 [-0.676, 0.978], mean action: 0.657 [0.000, 2.000], loss: 0.293946, mean_absolute_error: 17.845539, mean_q: 27.035683\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 26892/100000: episode: 27, duration: 3.352s, episode steps: 996, steps per second: 297, episode reward: 284.219, mean reward: 0.285 [-0.676, 0.978], mean action: 0.657 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.293946, mean_absolute_error: 17.845539, mean_q: 27.035683\n",
      "profit___282.89321335671485\n",
      " 27888/100000: episode: 28, duration: 3.317s, episode steps: 996, steps per second: 300, episode reward: 282.893, mean reward: 0.284 [-0.859, 0.978], mean action: 0.655 [0.000, 2.000], loss: 0.296379, mean_absolute_error: 17.972088, mean_q: 27.216761\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 27888/100000: episode: 28, duration: 3.318s, episode steps: 996, steps per second: 300, episode reward: 282.893, mean reward: 0.284 [-0.859, 0.978], mean action: 0.655 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.296379, mean_absolute_error: 17.972088, mean_q: 27.216761\n",
      "profit___290.64195806503994\n",
      " 28884/100000: episode: 29, duration: 3.338s, episode steps: 996, steps per second: 298, episode reward: 290.642, mean reward: 0.292 [-0.945, 0.978], mean action: 0.640 [0.000, 2.000], loss: 0.320379, mean_absolute_error: 18.129004, mean_q: 27.454172\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 28884/100000: episode: 29, duration: 3.339s, episode steps: 996, steps per second: 298, episode reward: 290.642, mean reward: 0.292 [-0.945, 0.978], mean action: 0.640 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.320379, mean_absolute_error: 18.129004, mean_q: 27.454172\n",
      "profit___285.2341097098547\n",
      " 29880/100000: episode: 30, duration: 3.460s, episode steps: 996, steps per second: 288, episode reward: 285.234, mean reward: 0.286 [-0.889, 0.978], mean action: 0.667 [0.000, 2.000], loss: 0.389587, mean_absolute_error: 18.224829, mean_q: 27.581991\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 29880/100000: episode: 30, duration: 3.461s, episode steps: 996, steps per second: 288, episode reward: 285.234, mean reward: 0.286 [-0.889, 0.978], mean action: 0.667 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.389587, mean_absolute_error: 18.224829, mean_q: 27.581991\n",
      "profit___281.87637324984246\n",
      " 30876/100000: episode: 31, duration: 3.400s, episode steps: 996, steps per second: 293, episode reward: 281.876, mean reward: 0.283 [-0.724, 0.978], mean action: 0.650 [0.000, 2.000], loss: 0.370327, mean_absolute_error: 18.266798, mean_q: 27.624392\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 30876/100000: episode: 31, duration: 3.401s, episode steps: 996, steps per second: 293, episode reward: 281.876, mean reward: 0.283 [-0.724, 0.978], mean action: 0.650 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.370327, mean_absolute_error: 18.266798, mean_q: 27.624392\n",
      "profit___274.1872902138461\n",
      " 31872/100000: episode: 32, duration: 3.390s, episode steps: 996, steps per second: 294, episode reward: 274.187, mean reward: 0.275 [-0.920, 0.978], mean action: 0.610 [0.000, 2.000], loss: 0.351827, mean_absolute_error: 18.336988, mean_q: 27.749769\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 31872/100000: episode: 32, duration: 3.391s, episode steps: 996, steps per second: 294, episode reward: 274.187, mean reward: 0.275 [-0.920, 0.978], mean action: 0.610 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.351827, mean_absolute_error: 18.336988, mean_q: 27.749769\n",
      "profit___287.2592821074177\n",
      " 32868/100000: episode: 33, duration: 3.402s, episode steps: 996, steps per second: 293, episode reward: 287.259, mean reward: 0.288 [-0.646, 0.978], mean action: 0.617 [0.000, 2.000], loss: 0.354989, mean_absolute_error: 18.396418, mean_q: 27.827822\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 32868/100000: episode: 33, duration: 3.403s, episode steps: 996, steps per second: 293, episode reward: 287.259, mean reward: 0.288 [-0.646, 0.978], mean action: 0.617 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.354989, mean_absolute_error: 18.396418, mean_q: 27.827822\n",
      "profit___280.02221744296196\n",
      " 33864/100000: episode: 34, duration: 3.422s, episode steps: 996, steps per second: 291, episode reward: 280.022, mean reward: 0.281 [-0.770, 0.978], mean action: 0.682 [0.000, 2.000], loss: 0.396052, mean_absolute_error: 18.445095, mean_q: 27.898014\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 33864/100000: episode: 34, duration: 3.423s, episode steps: 996, steps per second: 291, episode reward: 280.022, mean reward: 0.281 [-0.770, 0.978], mean action: 0.682 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.396052, mean_absolute_error: 18.445095, mean_q: 27.898014\n",
      "profit___271.3279148490194\n",
      " 34860/100000: episode: 35, duration: 3.371s, episode steps: 996, steps per second: 295, episode reward: 271.328, mean reward: 0.272 [-0.842, 0.971], mean action: 0.686 [0.000, 2.000], loss: 0.489032, mean_absolute_error: 18.409903, mean_q: 27.809141\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 34860/100000: episode: 35, duration: 3.372s, episode steps: 996, steps per second: 295, episode reward: 271.328, mean reward: 0.272 [-0.842, 0.971], mean action: 0.686 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.489032, mean_absolute_error: 18.409903, mean_q: 27.809141\n",
      "profit___278.378028642605\n",
      " 35856/100000: episode: 36, duration: 3.407s, episode steps: 996, steps per second: 292, episode reward: 278.378, mean reward: 0.279 [-0.671, 0.978], mean action: 0.663 [0.000, 2.000], loss: 0.535753, mean_absolute_error: 18.391783, mean_q: 27.770493\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 35856/100000: episode: 36, duration: 3.408s, episode steps: 996, steps per second: 292, episode reward: 278.378, mean reward: 0.279 [-0.671, 0.978], mean action: 0.663 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.535753, mean_absolute_error: 18.391783, mean_q: 27.770493\n",
      "profit___276.62514214654175\n",
      " 36852/100000: episode: 37, duration: 3.434s, episode steps: 996, steps per second: 290, episode reward: 276.625, mean reward: 0.278 [-0.742, 0.978], mean action: 0.682 [0.000, 2.000], loss: 0.470803, mean_absolute_error: 18.379200, mean_q: 27.741882\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 36852/100000: episode: 37, duration: 3.435s, episode steps: 996, steps per second: 290, episode reward: 276.625, mean reward: 0.278 [-0.742, 0.978], mean action: 0.682 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.470803, mean_absolute_error: 18.379200, mean_q: 27.741882\n",
      "profit___266.9247288653953\n",
      " 37848/100000: episode: 38, duration: 3.466s, episode steps: 996, steps per second: 287, episode reward: 266.925, mean reward: 0.268 [-0.941, 0.978], mean action: 0.621 [0.000, 2.000], loss: 0.389790, mean_absolute_error: 18.398512, mean_q: 27.776485\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 37848/100000: episode: 38, duration: 3.467s, episode steps: 996, steps per second: 287, episode reward: 266.925, mean reward: 0.268 [-0.941, 0.978], mean action: 0.621 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.389790, mean_absolute_error: 18.398512, mean_q: 27.776485\n",
      "profit___267.2214256872347\n",
      " 38844/100000: episode: 39, duration: 3.639s, episode steps: 996, steps per second: 274, episode reward: 267.221, mean reward: 0.268 [-0.770, 0.978], mean action: 0.681 [0.000, 2.000], loss: 0.445446, mean_absolute_error: 18.417107, mean_q: 27.790361\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 38844/100000: episode: 39, duration: 3.640s, episode steps: 996, steps per second: 274, episode reward: 267.221, mean reward: 0.268 [-0.770, 0.978], mean action: 0.681 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.445446, mean_absolute_error: 18.417107, mean_q: 27.790361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit___273.25701703304463\n",
      " 39840/100000: episode: 40, duration: 3.711s, episode steps: 996, steps per second: 268, episode reward: 273.257, mean reward: 0.274 [-0.862, 0.978], mean action: 0.652 [0.000, 2.000], loss: 0.447832, mean_absolute_error: 18.428686, mean_q: 27.823250\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 39840/100000: episode: 40, duration: 3.712s, episode steps: 996, steps per second: 268, episode reward: 273.257, mean reward: 0.274 [-0.862, 0.978], mean action: 0.652 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.447832, mean_absolute_error: 18.428686, mean_q: 27.823250\n",
      "profit___261.47756224909887\n",
      " 40836/100000: episode: 41, duration: 3.620s, episode steps: 996, steps per second: 275, episode reward: 261.478, mean reward: 0.263 [-0.944, 0.978], mean action: 0.708 [0.000, 2.000], loss: 0.444542, mean_absolute_error: 18.470444, mean_q: 27.863611\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 40836/100000: episode: 41, duration: 3.621s, episode steps: 996, steps per second: 275, episode reward: 261.478, mean reward: 0.263 [-0.944, 0.978], mean action: 0.708 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.444542, mean_absolute_error: 18.470444, mean_q: 27.863611\n",
      "profit___280.5912637293057\n",
      " 41832/100000: episode: 42, duration: 3.649s, episode steps: 996, steps per second: 273, episode reward: 280.591, mean reward: 0.282 [-0.786, 0.978], mean action: 0.664 [0.000, 2.000], loss: 0.333672, mean_absolute_error: 18.504158, mean_q: 27.932673\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 41832/100000: episode: 42, duration: 3.650s, episode steps: 996, steps per second: 273, episode reward: 280.591, mean reward: 0.282 [-0.786, 0.978], mean action: 0.664 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.333672, mean_absolute_error: 18.504158, mean_q: 27.932673\n",
      "profit___283.6039492375403\n",
      " 42828/100000: episode: 43, duration: 3.580s, episode steps: 996, steps per second: 278, episode reward: 283.604, mean reward: 0.285 [-0.766, 0.978], mean action: 0.651 [0.000, 2.000], loss: 0.329035, mean_absolute_error: 18.539598, mean_q: 27.994051\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 42828/100000: episode: 43, duration: 3.581s, episode steps: 996, steps per second: 278, episode reward: 283.604, mean reward: 0.285 [-0.766, 0.978], mean action: 0.651 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.329035, mean_absolute_error: 18.539598, mean_q: 27.994051\n",
      "profit___273.8875894873365\n",
      " 43824/100000: episode: 44, duration: 3.567s, episode steps: 996, steps per second: 279, episode reward: 273.888, mean reward: 0.275 [-0.767, 0.978], mean action: 0.644 [0.000, 2.000], loss: 0.382910, mean_absolute_error: 18.587067, mean_q: 28.065567\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 43824/100000: episode: 44, duration: 3.568s, episode steps: 996, steps per second: 279, episode reward: 273.888, mean reward: 0.275 [-0.767, 0.978], mean action: 0.644 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.382910, mean_absolute_error: 18.587067, mean_q: 28.065567\n",
      "profit___272.3204573820541\n",
      " 44820/100000: episode: 45, duration: 3.583s, episode steps: 996, steps per second: 278, episode reward: 272.320, mean reward: 0.273 [-0.934, 0.978], mean action: 0.667 [0.000, 2.000], loss: 0.387137, mean_absolute_error: 18.628353, mean_q: 28.122799\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 44820/100000: episode: 45, duration: 3.584s, episode steps: 996, steps per second: 278, episode reward: 272.320, mean reward: 0.273 [-0.934, 0.978], mean action: 0.667 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.387137, mean_absolute_error: 18.628353, mean_q: 28.122799\n",
      "profit___278.5391372440377\n",
      " 45816/100000: episode: 46, duration: 3.592s, episode steps: 996, steps per second: 277, episode reward: 278.539, mean reward: 0.280 [-0.668, 0.978], mean action: 0.677 [0.000, 2.000], loss: 0.370154, mean_absolute_error: 18.677797, mean_q: 28.197586\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 45816/100000: episode: 46, duration: 3.593s, episode steps: 996, steps per second: 277, episode reward: 278.539, mean reward: 0.280 [-0.668, 0.978], mean action: 0.677 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.370154, mean_absolute_error: 18.677797, mean_q: 28.197586\n",
      "profit___269.8500481084415\n",
      " 46812/100000: episode: 47, duration: 3.658s, episode steps: 996, steps per second: 272, episode reward: 269.850, mean reward: 0.271 [-0.934, 0.978], mean action: 0.649 [0.000, 2.000], loss: 0.382853, mean_absolute_error: 18.698912, mean_q: 28.238808\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 46812/100000: episode: 47, duration: 3.659s, episode steps: 996, steps per second: 272, episode reward: 269.850, mean reward: 0.271 [-0.934, 0.978], mean action: 0.649 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.382853, mean_absolute_error: 18.698912, mean_q: 28.238808\n",
      "profit___277.1785274140355\n",
      " 47808/100000: episode: 48, duration: 3.667s, episode steps: 996, steps per second: 272, episode reward: 277.179, mean reward: 0.278 [-0.934, 0.978], mean action: 0.735 [0.000, 2.000], loss: 0.347341, mean_absolute_error: 18.734255, mean_q: 28.304665\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 47808/100000: episode: 48, duration: 3.668s, episode steps: 996, steps per second: 272, episode reward: 277.179, mean reward: 0.278 [-0.934, 0.978], mean action: 0.735 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.347341, mean_absolute_error: 18.734255, mean_q: 28.304665\n",
      "profit___275.16830018450054\n",
      " 48804/100000: episode: 49, duration: 3.661s, episode steps: 996, steps per second: 272, episode reward: 275.168, mean reward: 0.276 [-0.811, 0.978], mean action: 0.703 [0.000, 2.000], loss: 0.335486, mean_absolute_error: 18.795715, mean_q: 28.408775\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 48804/100000: episode: 49, duration: 3.662s, episode steps: 996, steps per second: 272, episode reward: 275.168, mean reward: 0.276 [-0.811, 0.978], mean action: 0.703 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.335486, mean_absolute_error: 18.795715, mean_q: 28.408775\n",
      "profit___265.6102796512518\n",
      " 49800/100000: episode: 50, duration: 3.715s, episode steps: 996, steps per second: 268, episode reward: 265.610, mean reward: 0.267 [-0.934, 0.971], mean action: 0.704 [0.000, 2.000], loss: 0.358150, mean_absolute_error: 18.879017, mean_q: 28.537224\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 49800/100000: episode: 50, duration: 3.716s, episode steps: 996, steps per second: 268, episode reward: 265.610, mean reward: 0.267 [-0.934, 0.971], mean action: 0.704 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.358150, mean_absolute_error: 18.879017, mean_q: 28.537224\n",
      "profit___269.43656411478173\n",
      " 50796/100000: episode: 51, duration: 3.749s, episode steps: 996, steps per second: 266, episode reward: 269.437, mean reward: 0.271 [-0.971, 0.978], mean action: 0.696 [0.000, 2.000], loss: 0.340131, mean_absolute_error: 18.965321, mean_q: 28.673256\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 50796/100000: episode: 51, duration: 3.750s, episode steps: 996, steps per second: 266, episode reward: 269.437, mean reward: 0.271 [-0.971, 0.978], mean action: 0.696 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.340131, mean_absolute_error: 18.965321, mean_q: 28.673256\n",
      "profit___263.97655793284133\n",
      " 51792/100000: episode: 52, duration: 3.680s, episode steps: 996, steps per second: 271, episode reward: 263.977, mean reward: 0.265 [-0.934, 0.978], mean action: 0.694 [0.000, 2.000], loss: 0.401495, mean_absolute_error: 19.006609, mean_q: 28.729759\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 51792/100000: episode: 52, duration: 3.681s, episode steps: 996, steps per second: 271, episode reward: 263.977, mean reward: 0.265 [-0.934, 0.978], mean action: 0.694 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.401495, mean_absolute_error: 19.006609, mean_q: 28.729759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit___262.0331442373136\n",
      " 52788/100000: episode: 53, duration: 3.704s, episode steps: 996, steps per second: 269, episode reward: 262.033, mean reward: 0.263 [-0.934, 0.978], mean action: 0.733 [0.000, 2.000], loss: 0.367919, mean_absolute_error: 19.045084, mean_q: 28.793909\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 52788/100000: episode: 53, duration: 3.705s, episode steps: 996, steps per second: 269, episode reward: 262.033, mean reward: 0.263 [-0.934, 0.978], mean action: 0.733 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.367919, mean_absolute_error: 19.045084, mean_q: 28.793909\n",
      "profit___240.38341794506795\n",
      " 53784/100000: episode: 54, duration: 3.703s, episode steps: 996, steps per second: 269, episode reward: 240.383, mean reward: 0.241 [-0.944, 0.978], mean action: 0.772 [0.000, 2.000], loss: 0.433145, mean_absolute_error: 19.072876, mean_q: 28.832588\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 53784/100000: episode: 54, duration: 3.704s, episode steps: 996, steps per second: 269, episode reward: 240.383, mean reward: 0.241 [-0.944, 0.978], mean action: 0.772 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.433145, mean_absolute_error: 19.072876, mean_q: 28.832588\n",
      "profit___267.6344670749142\n",
      " 54780/100000: episode: 55, duration: 3.691s, episode steps: 996, steps per second: 270, episode reward: 267.634, mean reward: 0.269 [-0.934, 0.978], mean action: 0.682 [0.000, 2.000], loss: 0.325584, mean_absolute_error: 19.173063, mean_q: 29.012293\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 54780/100000: episode: 55, duration: 3.692s, episode steps: 996, steps per second: 270, episode reward: 267.634, mean reward: 0.269 [-0.934, 0.978], mean action: 0.682 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.325584, mean_absolute_error: 19.173063, mean_q: 29.012293\n",
      "profit___253.454851116978\n",
      " 55776/100000: episode: 56, duration: 3.698s, episode steps: 996, steps per second: 269, episode reward: 253.455, mean reward: 0.254 [-0.939, 0.978], mean action: 0.676 [0.000, 2.000], loss: 0.378519, mean_absolute_error: 19.247557, mean_q: 29.125170\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 55776/100000: episode: 56, duration: 3.699s, episode steps: 996, steps per second: 269, episode reward: 253.455, mean reward: 0.254 [-0.939, 0.978], mean action: 0.676 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.378519, mean_absolute_error: 19.247557, mean_q: 29.125170\n",
      "profit___260.50951188805243\n",
      " 56772/100000: episode: 57, duration: 3.609s, episode steps: 996, steps per second: 276, episode reward: 260.510, mean reward: 0.262 [-0.934, 0.978], mean action: 0.683 [0.000, 2.000], loss: 0.344980, mean_absolute_error: 19.336254, mean_q: 29.272335\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 56772/100000: episode: 57, duration: 3.610s, episode steps: 996, steps per second: 276, episode reward: 260.510, mean reward: 0.262 [-0.934, 0.978], mean action: 0.683 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.344980, mean_absolute_error: 19.336254, mean_q: 29.272335\n",
      "profit___265.73641526210343\n",
      " 57768/100000: episode: 58, duration: 3.597s, episode steps: 996, steps per second: 277, episode reward: 265.736, mean reward: 0.267 [-0.934, 0.978], mean action: 0.716 [0.000, 2.000], loss: 0.391737, mean_absolute_error: 19.377031, mean_q: 29.328192\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 57768/100000: episode: 58, duration: 3.598s, episode steps: 996, steps per second: 277, episode reward: 265.736, mean reward: 0.267 [-0.934, 0.978], mean action: 0.716 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.391737, mean_absolute_error: 19.377031, mean_q: 29.328192\n",
      "profit___254.7045173916775\n",
      " 58764/100000: episode: 59, duration: 3.604s, episode steps: 996, steps per second: 276, episode reward: 254.705, mean reward: 0.256 [-0.934, 0.978], mean action: 0.661 [0.000, 2.000], loss: 0.396367, mean_absolute_error: 19.444685, mean_q: 29.422642\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 58764/100000: episode: 59, duration: 3.605s, episode steps: 996, steps per second: 276, episode reward: 254.705, mean reward: 0.256 [-0.934, 0.978], mean action: 0.661 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.396367, mean_absolute_error: 19.444685, mean_q: 29.422642\n",
      "profit___260.26603864431695\n",
      " 59760/100000: episode: 60, duration: 3.572s, episode steps: 996, steps per second: 279, episode reward: 260.266, mean reward: 0.261 [-0.934, 0.978], mean action: 0.674 [0.000, 2.000], loss: 0.312983, mean_absolute_error: 19.528536, mean_q: 29.550030\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 59760/100000: episode: 60, duration: 3.574s, episode steps: 996, steps per second: 279, episode reward: 260.266, mean reward: 0.261 [-0.934, 0.978], mean action: 0.674 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.312983, mean_absolute_error: 19.528536, mean_q: 29.550030\n",
      "profit___254.4127341250498\n",
      " 60756/100000: episode: 61, duration: 3.582s, episode steps: 996, steps per second: 278, episode reward: 254.413, mean reward: 0.255 [-0.941, 0.978], mean action: 0.685 [0.000, 2.000], loss: 0.375293, mean_absolute_error: 19.562229, mean_q: 29.584267\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 60756/100000: episode: 61, duration: 3.583s, episode steps: 996, steps per second: 278, episode reward: 254.413, mean reward: 0.255 [-0.941, 0.978], mean action: 0.685 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.375293, mean_absolute_error: 19.562229, mean_q: 29.584267\n",
      "profit___275.5546818391879\n",
      " 61752/100000: episode: 62, duration: 3.607s, episode steps: 996, steps per second: 276, episode reward: 275.555, mean reward: 0.277 [-0.934, 0.978], mean action: 0.643 [0.000, 2.000], loss: 0.315818, mean_absolute_error: 19.547438, mean_q: 29.570162\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 61752/100000: episode: 62, duration: 3.608s, episode steps: 996, steps per second: 276, episode reward: 275.555, mean reward: 0.277 [-0.934, 0.978], mean action: 0.643 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.315818, mean_absolute_error: 19.547438, mean_q: 29.570162\n",
      "profit___256.39129333926434\n",
      " 62748/100000: episode: 63, duration: 3.589s, episode steps: 996, steps per second: 278, episode reward: 256.391, mean reward: 0.257 [-0.934, 0.978], mean action: 0.628 [0.000, 2.000], loss: 0.316345, mean_absolute_error: 19.570215, mean_q: 29.601690\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 62748/100000: episode: 63, duration: 3.590s, episode steps: 996, steps per second: 277, episode reward: 256.391, mean reward: 0.257 [-0.934, 0.978], mean action: 0.628 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.316345, mean_absolute_error: 19.570215, mean_q: 29.601690\n",
      "profit___262.3454948340279\n",
      " 63744/100000: episode: 64, duration: 3.579s, episode steps: 996, steps per second: 278, episode reward: 262.345, mean reward: 0.263 [-0.971, 0.978], mean action: 0.611 [0.000, 2.000], loss: 0.283127, mean_absolute_error: 19.622759, mean_q: 29.663929\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 63744/100000: episode: 64, duration: 3.581s, episode steps: 996, steps per second: 278, episode reward: 262.345, mean reward: 0.263 [-0.971, 0.978], mean action: 0.611 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.283127, mean_absolute_error: 19.622759, mean_q: 29.663929\n",
      "profit___264.5335678548898\n",
      " 64740/100000: episode: 65, duration: 3.713s, episode steps: 996, steps per second: 268, episode reward: 264.534, mean reward: 0.266 [-0.934, 0.978], mean action: 0.672 [0.000, 2.000], loss: 0.368552, mean_absolute_error: 19.577053, mean_q: 29.582748\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 64740/100000: episode: 65, duration: 3.714s, episode steps: 996, steps per second: 268, episode reward: 264.534, mean reward: 0.266 [-0.934, 0.978], mean action: 0.672 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.368552, mean_absolute_error: 19.577053, mean_q: 29.582748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit___261.4392559125526\n",
      " 65736/100000: episode: 66, duration: 3.703s, episode steps: 996, steps per second: 269, episode reward: 261.439, mean reward: 0.262 [-0.934, 0.978], mean action: 0.682 [0.000, 2.000], loss: 0.391232, mean_absolute_error: 19.533293, mean_q: 29.527191\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 65736/100000: episode: 66, duration: 3.704s, episode steps: 996, steps per second: 269, episode reward: 261.439, mean reward: 0.262 [-0.934, 0.978], mean action: 0.682 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.391232, mean_absolute_error: 19.533293, mean_q: 29.527191\n",
      "profit___275.63107713929514\n",
      " 66732/100000: episode: 67, duration: 3.746s, episode steps: 996, steps per second: 266, episode reward: 275.631, mean reward: 0.277 [-0.934, 0.978], mean action: 0.632 [0.000, 2.000], loss: 0.347654, mean_absolute_error: 19.485090, mean_q: 29.453693\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 66732/100000: episode: 67, duration: 3.747s, episode steps: 996, steps per second: 266, episode reward: 275.631, mean reward: 0.277 [-0.934, 0.978], mean action: 0.632 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.347654, mean_absolute_error: 19.485090, mean_q: 29.453693\n",
      "profit___271.1074505407035\n",
      " 67728/100000: episode: 68, duration: 3.701s, episode steps: 996, steps per second: 269, episode reward: 271.107, mean reward: 0.272 [-0.664, 0.978], mean action: 0.687 [0.000, 2.000], loss: 0.322056, mean_absolute_error: 19.472330, mean_q: 29.440670\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 67728/100000: episode: 68, duration: 3.702s, episode steps: 996, steps per second: 269, episode reward: 271.107, mean reward: 0.272 [-0.664, 0.978], mean action: 0.687 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.322056, mean_absolute_error: 19.472330, mean_q: 29.440670\n",
      "profit___265.8520732609827\n",
      " 68724/100000: episode: 69, duration: 3.691s, episode steps: 996, steps per second: 270, episode reward: 265.852, mean reward: 0.267 [-0.923, 0.978], mean action: 0.631 [0.000, 2.000], loss: 0.378070, mean_absolute_error: 19.434704, mean_q: 29.371469\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 68724/100000: episode: 69, duration: 3.693s, episode steps: 996, steps per second: 270, episode reward: 265.852, mean reward: 0.267 [-0.923, 0.978], mean action: 0.631 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.378070, mean_absolute_error: 19.434704, mean_q: 29.371469\n",
      "profit___275.1978190124269\n",
      " 69720/100000: episode: 70, duration: 3.689s, episode steps: 996, steps per second: 270, episode reward: 275.198, mean reward: 0.276 [-0.842, 0.978], mean action: 0.663 [0.000, 2.000], loss: 0.375168, mean_absolute_error: 19.357071, mean_q: 29.260506\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 69720/100000: episode: 70, duration: 3.690s, episode steps: 996, steps per second: 270, episode reward: 275.198, mean reward: 0.276 [-0.842, 0.978], mean action: 0.663 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.375168, mean_absolute_error: 19.357071, mean_q: 29.260506\n",
      "profit___261.57733086952175\n",
      " 70716/100000: episode: 71, duration: 3.739s, episode steps: 996, steps per second: 266, episode reward: 261.577, mean reward: 0.263 [-0.776, 0.978], mean action: 0.710 [0.000, 2.000], loss: 0.472851, mean_absolute_error: 19.291746, mean_q: 29.148813\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 70716/100000: episode: 71, duration: 3.740s, episode steps: 996, steps per second: 266, episode reward: 261.577, mean reward: 0.263 [-0.776, 0.978], mean action: 0.710 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.472851, mean_absolute_error: 19.291746, mean_q: 29.148813\n",
      "profit___271.8169806943238\n",
      " 71712/100000: episode: 72, duration: 3.706s, episode steps: 996, steps per second: 269, episode reward: 271.817, mean reward: 0.273 [-0.822, 0.978], mean action: 0.649 [0.000, 2.000], loss: 0.314425, mean_absolute_error: 19.231302, mean_q: 29.064838\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 71712/100000: episode: 72, duration: 3.707s, episode steps: 996, steps per second: 269, episode reward: 271.817, mean reward: 0.273 [-0.822, 0.978], mean action: 0.649 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.314425, mean_absolute_error: 19.231302, mean_q: 29.064838\n",
      "profit___268.71859460196254\n",
      " 72708/100000: episode: 73, duration: 3.739s, episode steps: 996, steps per second: 266, episode reward: 268.719, mean reward: 0.270 [-0.933, 0.978], mean action: 0.665 [0.000, 2.000], loss: 0.350965, mean_absolute_error: 19.209663, mean_q: 29.032579\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 72708/100000: episode: 73, duration: 3.741s, episode steps: 996, steps per second: 266, episode reward: 268.719, mean reward: 0.270 [-0.933, 0.978], mean action: 0.665 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.350965, mean_absolute_error: 19.209663, mean_q: 29.032579\n",
      "profit___263.62910415137713\n",
      " 73704/100000: episode: 74, duration: 3.624s, episode steps: 996, steps per second: 275, episode reward: 263.629, mean reward: 0.265 [-0.939, 0.978], mean action: 0.634 [0.000, 2.000], loss: 0.351547, mean_absolute_error: 19.120340, mean_q: 28.895489\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 73704/100000: episode: 74, duration: 3.625s, episode steps: 996, steps per second: 275, episode reward: 263.629, mean reward: 0.265 [-0.939, 0.978], mean action: 0.634 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.351547, mean_absolute_error: 19.120340, mean_q: 28.895489\n",
      "profit___258.0239170120071\n",
      " 74700/100000: episode: 75, duration: 3.626s, episode steps: 996, steps per second: 275, episode reward: 258.024, mean reward: 0.259 [-0.932, 0.978], mean action: 0.712 [0.000, 2.000], loss: 0.417547, mean_absolute_error: 18.994661, mean_q: 28.704121\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 74700/100000: episode: 75, duration: 3.627s, episode steps: 996, steps per second: 275, episode reward: 258.024, mean reward: 0.259 [-0.932, 0.978], mean action: 0.712 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.417547, mean_absolute_error: 18.994661, mean_q: 28.704121\n",
      "profit___263.4311921934108\n",
      " 75696/100000: episode: 76, duration: 3.595s, episode steps: 996, steps per second: 277, episode reward: 263.431, mean reward: 0.264 [-0.971, 0.978], mean action: 0.685 [0.000, 2.000], loss: 0.477176, mean_absolute_error: 18.887226, mean_q: 28.538515\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 75696/100000: episode: 76, duration: 3.596s, episode steps: 996, steps per second: 277, episode reward: 263.431, mean reward: 0.264 [-0.971, 0.978], mean action: 0.685 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.477176, mean_absolute_error: 18.887226, mean_q: 28.538515\n",
      "profit___264.00146628393384\n",
      " 76692/100000: episode: 77, duration: 3.588s, episode steps: 996, steps per second: 278, episode reward: 264.001, mean reward: 0.265 [-0.846, 0.978], mean action: 0.673 [0.000, 2.000], loss: 0.374327, mean_absolute_error: 18.816370, mean_q: 28.425562\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 76692/100000: episode: 77, duration: 3.590s, episode steps: 996, steps per second: 277, episode reward: 264.001, mean reward: 0.265 [-0.846, 0.978], mean action: 0.673 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.374327, mean_absolute_error: 18.816370, mean_q: 28.425562\n",
      "profit___274.56971649597176\n",
      " 77688/100000: episode: 78, duration: 3.587s, episode steps: 996, steps per second: 278, episode reward: 274.570, mean reward: 0.276 [-0.862, 0.978], mean action: 0.727 [0.000, 2.000], loss: 0.541521, mean_absolute_error: 18.726072, mean_q: 28.291126\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 77688/100000: episode: 78, duration: 3.588s, episode steps: 996, steps per second: 278, episode reward: 274.570, mean reward: 0.276 [-0.862, 0.978], mean action: 0.727 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.541521, mean_absolute_error: 18.726072, mean_q: 28.291126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit___273.4103869361664\n",
      " 78684/100000: episode: 79, duration: 3.604s, episode steps: 996, steps per second: 276, episode reward: 273.410, mean reward: 0.275 [-0.727, 0.978], mean action: 0.702 [0.000, 2.000], loss: 0.384266, mean_absolute_error: 18.669577, mean_q: 28.262581\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 78684/100000: episode: 79, duration: 3.605s, episode steps: 996, steps per second: 276, episode reward: 273.410, mean reward: 0.275 [-0.727, 0.978], mean action: 0.702 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.384266, mean_absolute_error: 18.669577, mean_q: 28.262581\n",
      "profit___280.949437560194\n",
      " 79680/100000: episode: 80, duration: 3.580s, episode steps: 996, steps per second: 278, episode reward: 280.949, mean reward: 0.282 [-0.826, 0.978], mean action: 0.662 [0.000, 2.000], loss: 0.305278, mean_absolute_error: 18.689735, mean_q: 28.323927\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 79680/100000: episode: 80, duration: 3.581s, episode steps: 996, steps per second: 278, episode reward: 280.949, mean reward: 0.282 [-0.826, 0.978], mean action: 0.662 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.305278, mean_absolute_error: 18.689735, mean_q: 28.323927\n",
      "profit___275.5253996261399\n",
      " 80676/100000: episode: 81, duration: 3.670s, episode steps: 996, steps per second: 271, episode reward: 275.525, mean reward: 0.277 [-0.933, 0.978], mean action: 0.719 [0.000, 2.000], loss: 0.414691, mean_absolute_error: 18.686378, mean_q: 28.288305\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 80676/100000: episode: 81, duration: 3.671s, episode steps: 996, steps per second: 271, episode reward: 275.525, mean reward: 0.277 [-0.933, 0.978], mean action: 0.719 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.414691, mean_absolute_error: 18.686378, mean_q: 28.288305\n",
      "profit___276.6605135860072\n",
      " 81672/100000: episode: 82, duration: 3.677s, episode steps: 996, steps per second: 271, episode reward: 276.661, mean reward: 0.278 [-0.889, 0.978], mean action: 0.743 [0.000, 2.000], loss: 0.494171, mean_absolute_error: 18.703924, mean_q: 28.318474\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 81672/100000: episode: 82, duration: 3.678s, episode steps: 996, steps per second: 271, episode reward: 276.661, mean reward: 0.278 [-0.889, 0.978], mean action: 0.743 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.494171, mean_absolute_error: 18.703924, mean_q: 28.318474\n",
      "profit___273.54536738345394\n",
      " 82668/100000: episode: 83, duration: 3.707s, episode steps: 996, steps per second: 269, episode reward: 273.545, mean reward: 0.275 [-0.932, 0.978], mean action: 0.762 [0.000, 2.000], loss: 0.398271, mean_absolute_error: 18.671831, mean_q: 28.275049\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 82668/100000: episode: 83, duration: 3.708s, episode steps: 996, steps per second: 269, episode reward: 273.545, mean reward: 0.275 [-0.932, 0.978], mean action: 0.762 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.398271, mean_absolute_error: 18.671831, mean_q: 28.275049\n",
      "profit___274.7546092139484\n",
      " 83664/100000: episode: 84, duration: 3.700s, episode steps: 996, steps per second: 269, episode reward: 274.755, mean reward: 0.276 [-0.750, 0.978], mean action: 0.674 [0.000, 2.000], loss: 0.434871, mean_absolute_error: 18.684399, mean_q: 28.309677\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 83664/100000: episode: 84, duration: 3.702s, episode steps: 996, steps per second: 269, episode reward: 274.755, mean reward: 0.276 [-0.750, 0.978], mean action: 0.674 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.434871, mean_absolute_error: 18.684399, mean_q: 28.309677\n",
      "profit___264.03154090535224\n",
      " 84660/100000: episode: 85, duration: 3.687s, episode steps: 996, steps per second: 270, episode reward: 264.032, mean reward: 0.265 [-0.941, 0.978], mean action: 0.688 [0.000, 2.000], loss: 0.391839, mean_absolute_error: 18.736885, mean_q: 28.362263\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 84660/100000: episode: 85, duration: 3.688s, episode steps: 996, steps per second: 270, episode reward: 264.032, mean reward: 0.265 [-0.941, 0.978], mean action: 0.688 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.391839, mean_absolute_error: 18.736885, mean_q: 28.362263\n",
      "profit___274.9751586398097\n",
      " 85656/100000: episode: 86, duration: 3.709s, episode steps: 996, steps per second: 269, episode reward: 274.975, mean reward: 0.276 [-0.719, 0.978], mean action: 0.714 [0.000, 2.000], loss: 0.456853, mean_absolute_error: 18.785078, mean_q: 28.415548\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 85656/100000: episode: 86, duration: 3.710s, episode steps: 996, steps per second: 268, episode reward: 274.975, mean reward: 0.276 [-0.719, 0.978], mean action: 0.714 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.456853, mean_absolute_error: 18.785078, mean_q: 28.415548\n",
      "profit___270.4574781090269\n",
      " 86652/100000: episode: 87, duration: 3.672s, episode steps: 996, steps per second: 271, episode reward: 270.457, mean reward: 0.272 [-0.945, 0.978], mean action: 0.660 [0.000, 2.000], loss: 0.454891, mean_absolute_error: 18.777695, mean_q: 28.361702\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 86652/100000: episode: 87, duration: 3.673s, episode steps: 996, steps per second: 271, episode reward: 270.457, mean reward: 0.272 [-0.945, 0.978], mean action: 0.660 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.454891, mean_absolute_error: 18.777695, mean_q: 28.361702\n",
      "profit___262.22295856780653\n",
      " 87648/100000: episode: 88, duration: 3.701s, episode steps: 996, steps per second: 269, episode reward: 262.223, mean reward: 0.263 [-0.929, 0.978], mean action: 0.691 [0.000, 2.000], loss: 0.402007, mean_absolute_error: 18.727287, mean_q: 28.315199\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 87648/100000: episode: 88, duration: 3.702s, episode steps: 996, steps per second: 269, episode reward: 262.223, mean reward: 0.263 [-0.929, 0.978], mean action: 0.691 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.402007, mean_absolute_error: 18.727287, mean_q: 28.315199\n",
      "profit___268.96247235626805\n",
      " 88644/100000: episode: 89, duration: 3.674s, episode steps: 996, steps per second: 271, episode reward: 268.962, mean reward: 0.270 [-0.945, 0.978], mean action: 0.630 [0.000, 2.000], loss: 0.380609, mean_absolute_error: 18.773394, mean_q: 28.395248\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 88644/100000: episode: 89, duration: 3.676s, episode steps: 996, steps per second: 271, episode reward: 268.962, mean reward: 0.270 [-0.945, 0.978], mean action: 0.630 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.380609, mean_absolute_error: 18.773394, mean_q: 28.395248\n",
      "profit___275.68136735734265\n",
      " 89640/100000: episode: 90, duration: 3.689s, episode steps: 996, steps per second: 270, episode reward: 275.681, mean reward: 0.277 [-0.784, 0.978], mean action: 0.624 [0.000, 2.000], loss: 0.251240, mean_absolute_error: 18.851633, mean_q: 28.564623\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 89640/100000: episode: 90, duration: 3.690s, episode steps: 996, steps per second: 270, episode reward: 275.681, mean reward: 0.277 [-0.784, 0.978], mean action: 0.624 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.251240, mean_absolute_error: 18.851633, mean_q: 28.564623\n",
      "profit___278.9954107063508\n",
      " 90636/100000: episode: 91, duration: 3.586s, episode steps: 996, steps per second: 278, episode reward: 278.995, mean reward: 0.280 [-0.724, 0.978], mean action: 0.634 [0.000, 2.000], loss: 0.327946, mean_absolute_error: 18.985147, mean_q: 28.723618\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 90636/100000: episode: 91, duration: 3.587s, episode steps: 996, steps per second: 278, episode reward: 278.995, mean reward: 0.280 [-0.724, 0.978], mean action: 0.634 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.327946, mean_absolute_error: 18.985147, mean_q: 28.723618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit___270.24594978910517\n",
      " 91632/100000: episode: 92, duration: 3.590s, episode steps: 996, steps per second: 277, episode reward: 270.246, mean reward: 0.271 [-0.862, 0.978], mean action: 0.706 [0.000, 2.000], loss: 0.251445, mean_absolute_error: 19.088770, mean_q: 28.910667\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 91632/100000: episode: 92, duration: 3.591s, episode steps: 996, steps per second: 277, episode reward: 270.246, mean reward: 0.271 [-0.862, 0.978], mean action: 0.706 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.251445, mean_absolute_error: 19.088770, mean_q: 28.910667\n",
      "profit___274.04368361237925\n",
      " 92628/100000: episode: 93, duration: 3.591s, episode steps: 996, steps per second: 277, episode reward: 274.044, mean reward: 0.275 [-0.933, 0.978], mean action: 0.644 [0.000, 2.000], loss: 0.316830, mean_absolute_error: 19.223812, mean_q: 29.114166\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 92628/100000: episode: 93, duration: 3.592s, episode steps: 996, steps per second: 277, episode reward: 274.044, mean reward: 0.275 [-0.933, 0.978], mean action: 0.644 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.316830, mean_absolute_error: 19.223812, mean_q: 29.114166\n",
      "profit___274.029690956091\n",
      " 93624/100000: episode: 94, duration: 3.576s, episode steps: 996, steps per second: 278, episode reward: 274.030, mean reward: 0.275 [-0.932, 0.978], mean action: 0.666 [0.000, 2.000], loss: 0.373217, mean_absolute_error: 19.290733, mean_q: 29.217680\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 93624/100000: episode: 94, duration: 3.578s, episode steps: 996, steps per second: 278, episode reward: 274.030, mean reward: 0.275 [-0.932, 0.978], mean action: 0.666 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.373217, mean_absolute_error: 19.290733, mean_q: 29.217680\n",
      "profit___270.87166424385805\n",
      " 94620/100000: episode: 95, duration: 3.586s, episode steps: 996, steps per second: 278, episode reward: 270.872, mean reward: 0.272 [-0.888, 0.978], mean action: 0.661 [0.000, 2.000], loss: 0.321590, mean_absolute_error: 19.397690, mean_q: 29.382114\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 94620/100000: episode: 95, duration: 3.588s, episode steps: 996, steps per second: 278, episode reward: 270.872, mean reward: 0.272 [-0.888, 0.978], mean action: 0.661 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.321590, mean_absolute_error: 19.397690, mean_q: 29.382114\n",
      "profit___283.9944838956435\n",
      " 95616/100000: episode: 96, duration: 3.603s, episode steps: 996, steps per second: 276, episode reward: 283.994, mean reward: 0.285 [-0.939, 0.978], mean action: 0.676 [0.000, 2.000], loss: 0.449443, mean_absolute_error: 19.490015, mean_q: 29.507589\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 95616/100000: episode: 96, duration: 3.604s, episode steps: 996, steps per second: 276, episode reward: 283.994, mean reward: 0.285 [-0.939, 0.978], mean action: 0.676 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.449443, mean_absolute_error: 19.490015, mean_q: 29.507589\n",
      "profit___275.4827715920177\n",
      " 96612/100000: episode: 97, duration: 3.598s, episode steps: 996, steps per second: 277, episode reward: 275.483, mean reward: 0.277 [-0.777, 0.978], mean action: 0.631 [0.000, 2.000], loss: 0.296971, mean_absolute_error: 19.527206, mean_q: 29.565630\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 96612/100000: episode: 97, duration: 3.599s, episode steps: 996, steps per second: 277, episode reward: 275.483, mean reward: 0.277 [-0.777, 0.978], mean action: 0.631 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.296971, mean_absolute_error: 19.527206, mean_q: 29.565630\n",
      "profit___280.96713228913836\n",
      " 97608/100000: episode: 98, duration: 3.586s, episode steps: 996, steps per second: 278, episode reward: 280.967, mean reward: 0.282 [-0.932, 0.978], mean action: 0.692 [0.000, 2.000], loss: 0.416291, mean_absolute_error: 19.594355, mean_q: 29.641691\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 97608/100000: episode: 98, duration: 3.587s, episode steps: 996, steps per second: 278, episode reward: 280.967, mean reward: 0.282 [-0.932, 0.978], mean action: 0.692 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.416291, mean_absolute_error: 19.594355, mean_q: 29.641691\n",
      "profit___277.15634306282647\n",
      " 98604/100000: episode: 99, duration: 3.678s, episode steps: 996, steps per second: 271, episode reward: 277.156, mean reward: 0.278 [-0.944, 0.978], mean action: 0.652 [0.000, 2.000], loss: 0.419651, mean_absolute_error: 19.617294, mean_q: 29.667753\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 98604/100000: episode: 99, duration: 3.679s, episode steps: 996, steps per second: 271, episode reward: 277.156, mean reward: 0.278 [-0.944, 0.978], mean action: 0.652 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.419651, mean_absolute_error: 19.617294, mean_q: 29.667753\n",
      "profit___278.3666578337629\n",
      " 99600/100000: episode: 100, duration: 3.656s, episode steps: 996, steps per second: 272, episode reward: 278.367, mean reward: 0.279 [-0.657, 0.978], mean action: 0.633 [0.000, 2.000], loss: 0.458377, mean_absolute_error: 19.664593, mean_q: 29.734056\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 99600/100000: episode: 100, duration: 3.657s, episode steps: 996, steps per second: 272, episode reward: 278.367, mean reward: 0.279 [-0.657, 0.978], mean action: 0.633 [0.000, 2.000], mean observation: 0.498 [0.000, 1.000], loss: 0.458377, mean_absolute_error: 19.664593, mean_q: 29.734056\n",
      "done, took 364.482 seconds\n",
      "done, took 364.482 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27d8d079320>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback= MyCallback(\"tmp\")\n",
    "agent_v6.fit(env, nb_steps=100000, visualize=False,\n",
    "                  verbose=2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_v6.load_weights(\"tmp/best_weight.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = []\n",
    "pl = []\n",
    "for i in range(3, len(df)):\n",
    "    obs = calc_observation(df, i, columns)\n",
    "    action = agent_v6.forward(obs)\n",
    "    profit = calc_profit(action, df, i)\n",
    "    al.append(action)\n",
    "    pl.append(profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pred\"] = pandas.Series(al).shift(2)\n",
    "df[\"profit\"] = pandas.Series(pl).shift(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>pred</th>\n",
       "      <th>profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064463</td>\n",
       "      <td>0.441743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649705</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.372289</td>\n",
       "      <td>0.749011</td>\n",
       "      <td>-0.377280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.190842</td>\n",
       "      <td>0.181702</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.630703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.510036</td>\n",
       "      <td>0.970349</td>\n",
       "      <td>-0.376723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.376723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.223740</td>\n",
       "      <td>0.027363</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.009140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.532533</td>\n",
       "      <td>0.801741</td>\n",
       "      <td>-0.460313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.460313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.635256</td>\n",
       "      <td>0.056010</td>\n",
       "      <td>0.196377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.196377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.008362</td>\n",
       "      <td>0.570236</td>\n",
       "      <td>-0.269208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.240967</td>\n",
       "      <td>0.896362</td>\n",
       "      <td>0.579246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.579246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.333310</td>\n",
       "      <td>0.313848</td>\n",
       "      <td>-0.561873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.561873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.646404</td>\n",
       "      <td>0.033793</td>\n",
       "      <td>-0.655396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.174802</td>\n",
       "      <td>0.024875</td>\n",
       "      <td>0.019462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.480141</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.612611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.851833</td>\n",
       "      <td>0.311817</td>\n",
       "      <td>0.149927</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.149927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.153297</td>\n",
       "      <td>0.670905</td>\n",
       "      <td>-0.331120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.331120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.586613</td>\n",
       "      <td>0.909066</td>\n",
       "      <td>0.540017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.631946</td>\n",
       "      <td>-0.517609</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.517609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.825276</td>\n",
       "      <td>-0.322453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.322453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.316644</td>\n",
       "      <td>0.204511</td>\n",
       "      <td>-0.077067</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.077067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.291709</td>\n",
       "      <td>0.161971</td>\n",
       "      <td>-0.303375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.724767</td>\n",
       "      <td>0.058364</td>\n",
       "      <td>0.112132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.248075</td>\n",
       "      <td>0.733633</td>\n",
       "      <td>0.129739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.129739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.286308</td>\n",
       "      <td>0.822853</td>\n",
       "      <td>0.666403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.666403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.310318</td>\n",
       "      <td>0.646882</td>\n",
       "      <td>-0.485557</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.485557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.365477</td>\n",
       "      <td>0.851460</td>\n",
       "      <td>-0.536545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.859716</td>\n",
       "      <td>0.681539</td>\n",
       "      <td>-0.336564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.146941</td>\n",
       "      <td>0.415160</td>\n",
       "      <td>-0.485983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.485983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.780884</td>\n",
       "      <td>0.870550</td>\n",
       "      <td>0.178177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.087784</td>\n",
       "      <td>0.207119</td>\n",
       "      <td>-0.268219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>0.793018</td>\n",
       "      <td>0.390842</td>\n",
       "      <td>-0.175063</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.175063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>0.073798</td>\n",
       "      <td>0.198665</td>\n",
       "      <td>-0.227658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>0.807370</td>\n",
       "      <td>0.715877</td>\n",
       "      <td>0.402176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.402176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0.821538</td>\n",
       "      <td>0.910434</td>\n",
       "      <td>-0.124867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>0.709230</td>\n",
       "      <td>0.157212</td>\n",
       "      <td>0.091493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.091493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>0.437566</td>\n",
       "      <td>0.098963</td>\n",
       "      <td>-0.088896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>0.428550</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>0.552018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.552018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0.891079</td>\n",
       "      <td>0.269655</td>\n",
       "      <td>0.338603</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.338603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.648066</td>\n",
       "      <td>0.706052</td>\n",
       "      <td>-0.339168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.339168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.571137</td>\n",
       "      <td>0.178335</td>\n",
       "      <td>0.621424</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.621424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.694583</td>\n",
       "      <td>0.870126</td>\n",
       "      <td>-0.057986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>0.227479</td>\n",
       "      <td>0.378976</td>\n",
       "      <td>0.392802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.392802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>0.173139</td>\n",
       "      <td>0.740474</td>\n",
       "      <td>-0.175544</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.175544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>0.163446</td>\n",
       "      <td>0.098525</td>\n",
       "      <td>-0.151497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.151497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>0.604427</td>\n",
       "      <td>0.575554</td>\n",
       "      <td>-0.567335</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.567335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>0.106224</td>\n",
       "      <td>0.751217</td>\n",
       "      <td>0.064922</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>0.562171</td>\n",
       "      <td>0.463121</td>\n",
       "      <td>0.028872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>0.626973</td>\n",
       "      <td>0.358826</td>\n",
       "      <td>-0.644993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.644993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0.357778</td>\n",
       "      <td>0.517478</td>\n",
       "      <td>0.099050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>0.396709</td>\n",
       "      <td>0.260789</td>\n",
       "      <td>0.268147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>0.132245</td>\n",
       "      <td>0.645958</td>\n",
       "      <td>-0.159700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>0.141759</td>\n",
       "      <td>0.828993</td>\n",
       "      <td>0.135920</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.135920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>0.490070</td>\n",
       "      <td>0.384321</td>\n",
       "      <td>-0.513713</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.513713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>0.229624</td>\n",
       "      <td>0.166965</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.687234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.383081</td>\n",
       "      <td>0.124880</td>\n",
       "      <td>0.105750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.386204</td>\n",
       "      <td>0.071142</td>\n",
       "      <td>0.062659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.040839</td>\n",
       "      <td>0.975283</td>\n",
       "      <td>0.258201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.219689</td>\n",
       "      <td>0.082047</td>\n",
       "      <td>0.315062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.973195</td>\n",
       "      <td>0.918076</td>\n",
       "      <td>-0.934444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.373434</td>\n",
       "      <td>0.137642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            a         b         c  pred    profit\n",
       "0    0.064463  0.441743  0.000000   NaN       NaN\n",
       "1    0.649705  0.019002  0.000000   NaN       NaN\n",
       "2    0.372289  0.749011 -0.377280   0.0       NaN\n",
       "3    0.190842  0.181702  0.630703   1.0  0.630703\n",
       "4    0.510036  0.970349 -0.376723   1.0  0.376723\n",
       "5    0.223740  0.027363  0.009140   1.0 -0.009140\n",
       "6    0.532533  0.801741 -0.460313   0.0  0.460313\n",
       "7    0.635256  0.056010  0.196377   1.0  0.196377\n",
       "8    0.008362  0.570236 -0.269208   0.0  0.269208\n",
       "9    0.240967  0.896362  0.579246   0.0  0.579246\n",
       "10   0.333310  0.313848 -0.561873   1.0 -0.561873\n",
       "11   0.646404  0.033793 -0.655396   0.0  0.655396\n",
       "12   0.174802  0.024875  0.019462   0.0  0.019462\n",
       "13   0.480141  0.811261  0.612611   0.0  0.612611\n",
       "14   0.851833  0.311817  0.149927   1.0  0.149927\n",
       "15   0.153297  0.670905 -0.331120   0.0  0.331120\n",
       "16   0.586613  0.909066  0.540017   1.0  0.540017\n",
       "17   0.554878  0.631946 -0.517609   1.0  0.517609\n",
       "18   0.521901  0.825276 -0.322453   1.0  0.322453\n",
       "19   0.316644  0.204511 -0.077067   1.0  0.077067\n",
       "20   0.291709  0.161971 -0.303375   0.0  0.303375\n",
       "21   0.724767  0.058364  0.112132   0.0  0.112132\n",
       "22   0.248075  0.733633  0.129739   1.0  0.129739\n",
       "23   0.286308  0.822853  0.666403   0.0 -0.666403\n",
       "24   0.310318  0.646882 -0.485557   1.0 -0.485557\n",
       "25   0.365477  0.851460 -0.536545   1.0  0.536545\n",
       "26   0.859716  0.681539 -0.336564   1.0  0.336564\n",
       "27   0.146941  0.415160 -0.485983   0.0  0.485983\n",
       "28   0.780884  0.870550  0.178177   1.0  0.178177\n",
       "29   0.087784  0.207119 -0.268219   1.0  0.268219\n",
       "..        ...       ...       ...   ...       ...\n",
       "970  0.793018  0.390842 -0.175063   1.0  0.175063\n",
       "971  0.073798  0.198665 -0.227658   0.0  0.227658\n",
       "972  0.807370  0.715877  0.402176   1.0  0.402176\n",
       "973  0.821538  0.910434 -0.124867   0.0  0.124867\n",
       "974  0.709230  0.157212  0.091493   1.0  0.091493\n",
       "975  0.437566  0.098963 -0.088896   0.0  0.088896\n",
       "976  0.428550  0.767717  0.552018   0.0  0.552018\n",
       "977  0.891079  0.269655  0.338603   1.0  0.338603\n",
       "978  0.648066  0.706052 -0.339168   0.0  0.339168\n",
       "979  0.571137  0.178335  0.621424   1.0  0.621424\n",
       "980  0.694583  0.870126 -0.057986   0.0  0.057986\n",
       "981  0.227479  0.378976  0.392802   1.0  0.392802\n",
       "982  0.173139  0.740474 -0.175544   1.0  0.175544\n",
       "983  0.163446  0.098525 -0.151497   1.0  0.151497\n",
       "984  0.604427  0.575554 -0.567335   2.0  0.567335\n",
       "985  0.106224  0.751217  0.064922   2.0  0.000000\n",
       "986  0.562171  0.463121  0.028872   1.0  0.000000\n",
       "987  0.626973  0.358826 -0.644993   0.0  0.644993\n",
       "988  0.357778  0.517478  0.099050   0.0  0.099050\n",
       "989  0.396709  0.260789  0.268147   1.0  0.268147\n",
       "990  0.132245  0.645958 -0.159700   0.0  0.159700\n",
       "991  0.141759  0.828993  0.135920   1.0  0.135920\n",
       "992  0.490070  0.384321 -0.513713   1.0  0.513713\n",
       "993  0.229624  0.166965 -0.687234   0.0  0.687234\n",
       "994  0.383081  0.124880  0.105750   0.0  0.105750\n",
       "995  0.386204  0.071142  0.062659   0.0  0.062659\n",
       "996  0.040839  0.975283  0.258201   0.0  0.258201\n",
       "997  0.219689  0.082047  0.315062   NaN       NaN\n",
       "998  0.973195  0.918076 -0.934444   NaN       NaN\n",
       "999  0.878853  0.373434  0.137642   NaN       NaN\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334.94402994906136, 296.80984002904404)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"c\"].abs().sum(), df[\"profit\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
